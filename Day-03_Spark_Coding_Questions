Sample Input:

01,I am Big Data ,12312356,25.02
34,Big data is very easy to learn,12565452,78.90
56,I am learning big data so that to be a good data engineer,67.89


Sample output we want after doing some transformations:
(I,25.02 + 67.89) => (I,92.91)
(am,25.02+67.89) => (am,92.91)
(big,25.02+78.90+67.89) => (big,171.81)


val rdd1 = sc.textFile("/user/cloudera/sparkinput/file1.csv")

input:

01,I am Big Data ,12312356,25.02
34,Big data is very easy to learn,12565452,78.90
56,I am learning big data so that to be a good data engineer,67.89

val rdd2 = rdd1.map(x=>(x.split(",")(1),x.split("3").toFloat))

output after doing map:

(I am Big Data,25.02) 
(Big data is very easy to learn,78.90) 
(I am learning big data so that to be a good data engineer,67.89)

val rdd3 = rdd2.map(x=>(x.split(",")(3).toFloat,x.split(",")(1)))

output after doing map for second time:

(25.02,I am Big Data)
(78.9,Big data is very easy to learn)
(67.89,I am learning big data so that to be a good data engineer)

val rdd4 = rdd3.flatMapValues(x=>x.split(" "))

output after doing flatMapValues:
(25.02,I) (25.02,am) (25.02,Big) (25.02,Data)
(78.9,Big) (78.9,data) (78.9,is) (78.9,easy) (78.9,to) (78.9,learn)
(67.89,I) (67.89,am) (67.89,learning) (67.89,big) (67.89,data) 

val rdd5 = rdd4.map(x=>(x._2.toLowerCase(),x._1))

output after doing map for 3rd Time:
(i,25.02) (am,25.02) (big,25.02) (data,25.02)
(big,78.9) (data,78.9) (is,78.9) (easy,78.9) (to,78.9) (learn,78.9)
(i,67.89) (am,67.89) (learning,67.89) (big,67.89) (data,67.89)

val rdd6 = rdd5.reduceByKey((x,y) =>x+y)

output after doing reduceByKey:

(i,92.91) (am,92.91) (big,171.81) and so on ......




#########################################################################################################
#########################################################################################################

Q6)***

Sample dataset for Log File:

"WARN: Tuesday 4th sep 2023 0408"
"WARN: Tuesday 5th Sep 2023 0897"
"ERROR: Thursday 8th Nov 2022 0987"
"INFO: Friday 8th July 2022 0965"
"WARN: Tuesday 11th Jan 2021 0557"

Problem statement:
q1)We want to first create a list from the above data 
q2)We want to create an rdd out the list that we create 
q3)We want to calculate the count of WARN,INFO,ERROR  


Code In scala:

sol1)Creating List from the sample data 

val myList = LIST("WARN: Tuesday 4th sep 2023 0408",
                "WARN: Tuesday 5th Sep 2023 0897",
                "ERROR: Thursday 8th Nov 2022 0987",
                "INFO: Friday 8th July 2022 0965",
                "WARN: Tuesday 11th Jan 2021 0557")

sol2) creating rdd from collection
val rdd1 = sc.parallelize(myList)


Sol3)

val rdd2 = rdd1.map(x=>{
	                    val columns = x.split(":")
	                    val logLevel = columns(0)
	                    (logLevel,1)
                    })

val rdd3 = rdd2.reduceByKey((x,y)=> x+y)

rdd3.saveAsTextFile("/user/cloudera/sparkoutput/")

Final Output after transaformation:
(WARN,3) (INFO,1) (ERROR,1)


Code in pyspark:
=================
# Sol1: Creating List from the sample data
myList = ["WARN: Tuesday 4th sep 2023 0408",
          "WARN: Tuesday 5th Sep 2023 0897",
          "ERROR: Thursday 8th Nov 2022 0987",
          "INFO: Friday 8th July 2022 0965",
          "WARN: Tuesday 11th Jan 2021 0557"]

# Sol2: Creating RDD from collection
rdd1 = sc.parallelize(myList)

# Sol3: Mapping each line of RDD, extracting log level, and mapping to (logLevel, 1)
rdd2 = rdd1.map(lambda x: (x.split(":")[0], 1))

# Reducing by key to count occurrences of each log level
rdd3 = rdd2.reduceByKey(lambda x, y: x + y)

# Saving the RDD to text file
rdd3.saveAsTextFile("/user/cloudera/sparkoutput/")
